\documentclass[11pt,nocut]{article}

\usepackage{../latex_style/packages}
\usepackage{../latex_style/notations}

\title{\vspace{-2.0cm}%
	Optimization and Computational Linear Algebra for Data Science\\
Homework 3: Rank}
\date{\vspace{-1cm}Due on September 24, 2019}
\setcounter{section}{3}

\begin{document}
\maketitle
\input{./preamble_homeworks.tex}


\begin{problem}[2 points]
	Let $A \in \R^{n \times n}$.
	\begin{enumerate}[label=\normalfont(\textbf{\alph*})]
	% 3.1(a)
		\item Show that if $A = \alpha \Id_n$ for some $\alpha \in \R$, then for all $B \in \R^{n \times n}$ we have $A B = B A$.
		
		\begin{proof}
		Given: 
		$$
		A , B \in \R^{nxn}
		$$
		We see that:
		$$
		AB = BA
		$$
		Given $\alpha \in \R$ such that $A = \alpha \Id_n$, and that B is a square, full rank matrix:
		$$
		B \Id_n = B = \Id_n B
		$$
		Therefore:
		$$
		B (\alpha \Id_n) = (\alpha \Id_n) B
		$$
		$$
		\alpha (BA) = \alpha (AB)
		$$
		\end{proof}
		
	% 3.1(b) FINISH
		\item Conversely, show that if for all $B \in \R^{n \times n}$ we have $AB = BA$, then there exists $\alpha \in \R$ such that $A = \alpha \Id_n$.
	\end{enumerate}
	
		\begin{proof}
		Given: 
		$$
		A , B \in \R^{nxn}
		$$
		We see that:
		$$
		AB = BA
		$$
		Given that B is a square, full rank matrix:
		$$
		B \Id_n = B = \Id_n B
		$$
		Therefore \textbf{either} an $\alpha \Id_n$ exists such that $A = \alpha \Id_n$:
		$$
		B (\alpha \Id_n) = (\alpha \Id_n) B
		$$
		$$
		\alpha (BA) = \alpha (AB)
		$$
		\textbf{Or} no $\alpha \Id_n$ exists such that $A = \alpha \Id_n$, intuitively:
		$$
		B (0) = 0
		$$
		Thus, in any non-trivial case, an $\alpha \in \R$ must exist such that $A = \alpha \Id_n$. \\
		To more rigorously (actually) prove this you'd probably want to use contradiction or induction...
		\end{proof}
		
\end{problem}

\vspace{1mm}

\begin{problem}[2 points]
	Let $M \in \R^{n \times m}$ and $r = \rank(M)$. Show that there exists $A \in \R^{n \times r}$ and $B \in \R^{r \times m}$ such that $M = AB$. \\ \\
	% 3.2
	\vspace{1mm}
	\begin{proof} \\
	Given that we know $r = \rank(M)$:\\ 
	We know that we can construct a matrix $A$ by isolating all linearly independent columns from M, such that $A \in \R^{n \times r}$. (A will be directly proportional to the basis of M.)\\
	Then, if we then construct $B$, such that every column of $B$ is a linear combination of $A$, we can construct any column $M_{n,k}$ where $k \in \R, 1 \leq k \leq m$. Thus there exists an $A \in \R^{n \times r}$ and $B \in \R^{r \times m}$ such that $M = AB$.
	\end{proof}
\end{problem}

\vspace{1mm}

\begin{problem}[3 points]
Let $A \in \R^{n \times m}$. 
	\begin{enumerate}[label=\normalfont(\textbf{\alph*})]
		\item Let $M \in \R^{m \times m}$ be an invertible matrix.
			Show that
			$$
			\rank(AM) = \rank(A).
			$$
	% 3.3(a)
		\begin{proof}
		Given that M is invertible we know that:
		$$
		(M^T)^{-1} = (M^{-1})^T \mbox{ and }\rank(M) = \rank(M^T)
		$$
		Thus: $\rank(AM) = \rank((AM)^T) = \rank(A^TM^T) = \rank(A^T) = \rank(A)$
		\end{proof}
		\item Let $M \in \R^{n \times n}$ be an invertible matrix.
			Show that
			$$
			\rank(MA) = \rank(A).
			$$
	% 3.3(b)
		\begin{proof}
		Given that M is invertible we know that:
		$$
		M M^{-1} = \Id_n \mbox{, } \rank(M) = n \mbox{ and, } \Ker(M) = \{0\}
		$$
		Let $x \in \Ker(MA), M A x = 0$, therefore: $A x = 0$.
		$$
		x \in \Ker(A) \mbox{ means that } \Ker(MA)) \subseteq \Ker(A)
		$$
		$$
		\mbox{thus: } x \in \Ker(A) \Rightarrow Ax = 0 \Rightarrow M Ax = 0 \Rightarrow x \in \Ker(MA)
		$$
		$$
		\Ker(MA) = \Ker(A)
		$$
		Via the rank-nullity theorem, we know that: 
		$$
		\rank(MA)+ \Ker(MA) = n
		$$
		$$
		\rank(A) + \Ker(A) = n
		$$
		Since $\Ker(MA) = \Ker(A)$ it follows that $\rank(MA) = \rank(A)$
		\end{proof}
\end{enumerate}
\end{problem}

\vspace{1mm}

\begin{problem}[3 points]
	Let $A \in \R^{n \times n}$ be an ``upper triangular matrix'', i.e.\ a matrix of the form
	$$
	A=
	\begin{pmatrix}
		a_{1,1}&a_{1,2}&a_{1,3}&\ldots& a_{1,n}\\
		0 & a_{2,2}&a_{2,3}&\ldots& a_{2,n}\\
			   \vdots & &\ddots &\ddots &\vdots \\
			   \vdots & & & \ddots & a_{n-1,n}\\
		0& \cdots& \cdots & 0 &a_{n,n}
	\end{pmatrix}.
	$$
	Show that $A$ is invertible \emph{if and only if} its diagonal coefficients $a_{1,1}, a_{2,2}, \dots, a_{n,n}$ are all non-zero.\\ \\
	% 3.4
	\begin{proof}
	Given $A \in \R^{n \times n}$ is an invertible upper triangle matrix. \\
	Via contradiction, suppose that A has a diagonal coefficient $a_{k,k}$ equal to zero, such that $Ax = 0$:
	$$
	a_{k,k} = 0 \mbox{ where } k \in \R, 1 \leq k < n
	$$
	$$
		A=
	\begin{pmatrix}
		a_{1,1}	& 		& \cdots	& 		& a_{1,n}\\
		0 		& \ddots	& 		& 		& \\
		\vdots 	& 		& a_{k,k} 	& 		&\vdots \\
		 		& 		& 		& \ddots 	& \\
		0		& 		& \cdots 	& 0 		&a_{n,n}
	\end{pmatrix}.
	\begin{pmatrix}
		x_1\\
		\vdots \\
		x_k\\
		\vdots \\
		x_n
	\end{pmatrix} = 0
	$$
	We see the $k^{th}$ row, by having a 0 on the main diagonal row, has a nontrivial (0) solution:
	$$
	\underbrace {\overbrace {a_{k-1,k-1}x_{k-1}}^{k \neq 1}+a_{k-1,k}x_k}_{i} + \underbrace {a_{k-1,k+1}x_{k+t} + \cdots + a_{k-1,n}x_n}_{0} = 0
	$$
	Since $i$ implies a nontrivial solution, when $Ax = 0$ should only contain a trivial solution, we see via contradiction that if A is invertible if and only if its diagonal coefficients are all non-zero. 
	\end{proof}
\end{problem}


\begin{problem}[$\star$]
	The trace $\Tr(M)$ of a $k \times k$ matrix $M$ is defined as the sum of its diagonal coefficients, i.e.
	$$
	\Tr(M) = \sum_{i=1}^k M_{i,i}.
	$$
	Let $A \in \R^{n \times m}$ and $B \in \R^{m \times n}$. Show that
	$$
	\Tr(AB) = \Tr(BA).
	$$
	% 3.5
	\begin{proof}
	$$
	\begin{aligned}
	\Tr(AB) 	&= \sum_{i=1}^{n}(AB)_{ii}\\
			&= \sum_{i=1}^n \sum_{j=1}^m A_{ij}B_{ji} \\
			&= \sum_{j=1}^m \sum_{i=1}^n B_{ji}A_{ij} \\
			&= \sum_{j=1}^m(BA)_{jj} \\
			&= \Tr(BA)
	\end{aligned}
	$$
	\end{proof}
\end{problem}
\vspace{1cm}
\centerline{\pgfornament[width=7cm]{87}}

%\bibliographystyle{plain}
%\bibliography{./references.bib}
\end{document}